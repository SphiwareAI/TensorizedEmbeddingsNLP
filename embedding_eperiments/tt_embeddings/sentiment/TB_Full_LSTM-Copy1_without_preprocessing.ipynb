{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--embedding', \n",
    "    default='tt',\n",
    "    choices=['tt', 'tr', 'full'],\n",
    "    type=str)\n",
    "parser.add_argument('--ranks', type=int, default=8)\n",
    "parser.add_argument('--d', type=int, default=3)\n",
    "parser.add_argument('--embed_dim', type=int, default=64)\n",
    "parser.add_argument('--voc_dim', default=250, type=int)\n",
    "parser.add_argument('--lr', default=5e-4)\n",
    "parser.add_argument('--gpu', default='', type=str)\n",
    "parser.add_argument('--hidden_dim', default=128, type=int)\n",
    "parser.add_argument('--n_epochs',  default=100, type=int)\n",
    "parser.add_argument('--fout',  default=\"logdir/\", type=str)\n",
    "parser.add_argument('--dropout', default=0.5, type=float)\n",
    "parser.add_argument(\n",
    "    '--dataset',\n",
    "    default='Pcam',\n",
    "    type=str)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embedding == 'tt':\n",
    "    tt = \"tt\"\n",
    "elif args.embedding == 'tr':\n",
    "    tt = 'tr'\n",
    "else:             \n",
    "    tt = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"{args.dataset}-dim_{args.embed_dim}-d_{args.d}-ranks_{args.ranks}-{tt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=args.gpu\n",
    "import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import t3nsor as t3\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.optim as optim\n",
    "from models import LSTM_Classifier\n",
    "#from utils import binary_accuracy, train, evaluate\n",
    "import pickle\n",
    "import random\n",
    "import spacy\n",
    "from spacy.cli.download import download\n",
    "random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', fix_length=100)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "fields = [('sequence', TEXT),('label', LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Building the dataset...\")\n",
    "OUTPUT_DIM = 1000\n",
    "fine_grained = (OUTPUT_DIM == 1000)\n",
    "seq_train = \"/home/mashjunior/Downloads/Pcam_data/new_sequence_data/seq_train.csv\"\n",
    "seq_val = \"/home/mashjunior/Downloads/Pcam_data/new_sequence_data/seq_val.csv\"\n",
    "seq_test = \"/home/mashjunior/Downloads/Pcam_data/new_sequence_data/seq_test.csv\"\n",
    "    \n",
    "training_data = data.TabularDataset(\n",
    "    path=seq_train,\n",
    "    format='csv',\n",
    "    fields=fields,\n",
    "    skip_header=True,\n",
    ")\n",
    "\n",
    "validation_data = data.TabularDataset(\n",
    "    path=seq_val,\n",
    "    format='csv',\n",
    "    fields=fields,\n",
    "    skip_header=True,\n",
    ")\n",
    "\n",
    "test_data = data.TabularDataset(\n",
    "    path=seq_test,\n",
    "    format='csv',\n",
    "    fields=fields,\n",
    "    skip_header=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(ex):\n",
    "    return len(ex.sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(training_data, max_size=args.voc_dim - 2)\n",
    "LABEL.build_vocab(training_data)\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (training_data, validation_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)\n",
    "\n",
    "valid_iterator.sort_key = sort_key\n",
    "test_iterator.sort_key = sort_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = args.embed_dim\n",
    "HIDDEN_DIM = args.hidden_dim\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = args.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vocab_size = len(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTM_Classifier(embedding_dim=EMBEDDING_DIM,\n",
    "                             hidden_dim=HIDDEN_DIM,\n",
    "                             output_dim=OUTPUT_DIM,\n",
    "                             n_layers=N_LAYERS,\n",
    "                             bidirectional=BIDIRECTIONAL,\n",
    "                             dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embedding == 'tt':\n",
    "        embed_model = t3.TTEmbedding(\n",
    "            voc_size=INPUT_DIM,\n",
    "            emb_size=EMBEDDING_DIM,\n",
    "            auto_shapes=True,\n",
    "            auto_shape_mode='mixed',\n",
    "            d=args.d,\n",
    "            tt_rank=args.ranks,\n",
    "            padding_idx=1\n",
    "        )\n",
    "        compression_rate = INPUT_DIM * EMBEDDING_DIM / embed_model.tt_matrix.dof\n",
    "elif args.embedding == 'tr':\n",
    "        embed_model = t3.TREmbedding(\n",
    "            voc_size=INPUT_DIM,\n",
    "            emb_size=EMBEDDING_DIM,\n",
    "            auto_shapes=True,\n",
    "            auto_shape_mode='mixed',\n",
    "            d=args.d,\n",
    "            tr_rank=args.ranks,\n",
    "            padding_idx=1\n",
    "        )\n",
    "        compression_rate = INPUT_DIM * EMBEDDING_DIM / embed_model.tr_matrix.dof\n",
    "else:\n",
    "    embed_model = nn.Embedding(\n",
    "        num_embeddings=INPUT_DIM,\n",
    "        embedding_dim=EMBEDDING_DIM\n",
    "    )\n",
    "    compression_rate = 1.0\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, target):\n",
    "    labels = target.type(torch.LongTensor).to(logits.device)\n",
    "    return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(embed_model, lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    if len(preds.shape) == 1:\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    else:\n",
    "        rounded_preds = preds.argmax(1)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        dtype = torch.LongTensor\n",
    "    elif isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        device = batch.sequence.device\n",
    "        labels = batch.label.type(dtype).to(device)\n",
    "        predictions = model(batch.sequence).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        B = batch.label.shape[0]\n",
    "\n",
    "        epoch_loss += B * loss.item()\n",
    "        epoch_acc += B * acc.item()\n",
    "\n",
    "        total_len += B\n",
    "\n",
    "\n",
    "        if i > len(iterator):\n",
    "            break\n",
    "\n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        dtype = torch.LongTensor\n",
    "    elif isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            device = batch.sequence.device\n",
    "            labels = batch.label.type(dtype).to(device)\n",
    "            predictions = model(batch.sequence).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            B = batch.label.shape[0]\n",
    "\n",
    "            epoch_loss += B * loss.item()\n",
    "            epoch_acc += B * acc.item()\n",
    "            total_len += B\n",
    "\n",
    "            if i > len(iterator):\n",
    "                break\n",
    "\n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): TTEmbedding(\n",
      "    (parameters): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 1x5x4x8]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 8x6x4x8]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 8x10x4x1]\n",
      "    )\n",
      "  )\n",
      "  (1): LSTM_Classifier(\n",
      "    (rnn): LSTM(64, 128, dropout=0.5, bidirectional=True)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "print(model)\n",
    "N_EPOCHS = args.n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {\n",
    "    'compression_rate':compression_rate,\n",
    "    'train_loss':[], 'test_loss':[], 'valid_loss':[],\n",
    "    'train_acc':[], 'test_acc':[], 'valid_acc':[]}\n",
    "best_result = {\n",
    "    \"epoch\": 0, \"train_acc\": 0, \"valid_acc\": 0, \"train_acc\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 1.00 | Train Loss: 6.173 | Train Acc: 1.60% | Val. Loss: 5.590 | Val. Acc: 3.21% | Test Loss: 5.590 | Test Acc: 3.21% |\n",
      "TEST ACCURACY: 3.21\n",
      "| Epoch: 2.00 | Train Loss: 5.529 | Train Acc: 3.42% | Val. Loss: 5.280 | Val. Acc: 4.74% | Test Loss: 5.280 | Test Acc: 4.74% |\n",
      "TEST ACCURACY: 4.74\n",
      "| Epoch: 3.00 | Train Loss: 5.326 | Train Acc: 4.35% | Val. Loss: 5.118 | Val. Acc: 6.32% | Test Loss: 5.118 | Test Acc: 6.32% |\n",
      "TEST ACCURACY: 6.32\n",
      "| Epoch: 4.00 | Train Loss: 5.212 | Train Acc: 5.21% | Val. Loss: 5.067 | Val. Acc: 6.66% | Test Loss: 5.067 | Test Acc: 6.66% |\n",
      "TEST ACCURACY: 6.66\n",
      "| Epoch: 5.00 | Train Loss: 5.135 | Train Acc: 5.91% | Val. Loss: 4.961 | Val. Acc: 7.67% | Test Loss: 4.961 | Test Acc: 7.67% |\n",
      "TEST ACCURACY: 7.67\n",
      "| Epoch: 6.00 | Train Loss: 5.046 | Train Acc: 6.74% | Val. Loss: 4.994 | Val. Acc: 6.47% | Test Loss: 4.994 | Test Acc: 6.47% |\n",
      "TEST ACCURACY: 7.67\n",
      "| Epoch: 7.00 | Train Loss: 5.007 | Train Acc: 7.40% | Val. Loss: 4.858 | Val. Acc: 9.36% | Test Loss: 4.858 | Test Acc: 9.36% |\n",
      "TEST ACCURACY: 9.36\n",
      "| Epoch: 8.00 | Train Loss: 4.972 | Train Acc: 7.91% | Val. Loss: 4.841 | Val. Acc: 9.83% | Test Loss: 4.841 | Test Acc: 9.83% |\n",
      "TEST ACCURACY: 9.83\n",
      "| Epoch: 9.00 | Train Loss: 4.957 | Train Acc: 8.29% | Val. Loss: 4.841 | Val. Acc: 9.91% | Test Loss: 4.841 | Test Acc: 9.91% |\n",
      "TEST ACCURACY: 9.91\n",
      "| Epoch: 10.00 | Train Loss: 4.938 | Train Acc: 8.70% | Val. Loss: 4.793 | Val. Acc: 10.63% | Test Loss: 4.793 | Test Acc: 10.63% |\n",
      "TEST ACCURACY: 10.63\n",
      "| Epoch: 11.00 | Train Loss: 4.899 | Train Acc: 9.19% | Val. Loss: 4.781 | Val. Acc: 10.58% | Test Loss: 4.781 | Test Acc: 10.58% |\n",
      "TEST ACCURACY: 10.63\n",
      "| Epoch: 12.00 | Train Loss: 4.881 | Train Acc: 9.50% | Val. Loss: 4.758 | Val. Acc: 11.56% | Test Loss: 4.758 | Test Acc: 11.56% |\n",
      "TEST ACCURACY: 11.56\n",
      "| Epoch: 13.00 | Train Loss: 4.867 | Train Acc: 9.98% | Val. Loss: 4.751 | Val. Acc: 10.55% | Test Loss: 4.751 | Test Acc: 10.55% |\n",
      "TEST ACCURACY: 11.56\n",
      "| Epoch: 14.00 | Train Loss: 4.853 | Train Acc: 10.20% | Val. Loss: 4.729 | Val. Acc: 11.67% | Test Loss: 4.729 | Test Acc: 11.67% |\n",
      "TEST ACCURACY: 11.67\n",
      "| Epoch: 15.00 | Train Loss: 4.838 | Train Acc: 10.31% | Val. Loss: 4.711 | Val. Acc: 11.78% | Test Loss: 4.711 | Test Acc: 11.78% |\n",
      "TEST ACCURACY: 11.78\n",
      "| Epoch: 16.00 | Train Loss: 4.833 | Train Acc: 10.45% | Val. Loss: 4.705 | Val. Acc: 12.12% | Test Loss: 4.705 | Test Acc: 12.12% |\n",
      "TEST ACCURACY: 12.12\n",
      "| Epoch: 17.00 | Train Loss: 4.820 | Train Acc: 10.58% | Val. Loss: 4.715 | Val. Acc: 11.54% | Test Loss: 4.715 | Test Acc: 11.54% |\n",
      "TEST ACCURACY: 12.12\n",
      "| Epoch: 18.00 | Train Loss: 4.810 | Train Acc: 10.81% | Val. Loss: 4.689 | Val. Acc: 12.19% | Test Loss: 4.689 | Test Acc: 12.19% |\n",
      "TEST ACCURACY: 12.19\n",
      "| Epoch: 19.00 | Train Loss: 4.804 | Train Acc: 10.93% | Val. Loss: 4.710 | Val. Acc: 11.33% | Test Loss: 4.710 | Test Acc: 11.33% |\n",
      "TEST ACCURACY: 12.19\n",
      "| Epoch: 20.00 | Train Loss: 4.810 | Train Acc: 10.83% | Val. Loss: 4.679 | Val. Acc: 12.59% | Test Loss: 4.679 | Test Acc: 12.59% |\n",
      "TEST ACCURACY: 12.59\n",
      "| Epoch: 21.00 | Train Loss: 4.789 | Train Acc: 11.15% | Val. Loss: 4.685 | Val. Acc: 12.31% | Test Loss: 4.685 | Test Acc: 12.31% |\n",
      "TEST ACCURACY: 12.59\n",
      "| Epoch: 22.00 | Train Loss: 4.782 | Train Acc: 11.41% | Val. Loss: 4.666 | Val. Acc: 12.99% | Test Loss: 4.666 | Test Acc: 12.99% |\n",
      "TEST ACCURACY: 12.99\n",
      "| Epoch: 23.00 | Train Loss: 4.778 | Train Acc: 11.52% | Val. Loss: 4.668 | Val. Acc: 12.86% | Test Loss: 4.668 | Test Acc: 12.86% |\n",
      "TEST ACCURACY: 12.99\n",
      "| Epoch: 24.00 | Train Loss: 4.776 | Train Acc: 11.35% | Val. Loss: 4.660 | Val. Acc: 12.89% | Test Loss: 4.660 | Test Acc: 12.89% |\n",
      "TEST ACCURACY: 12.99\n",
      "| Epoch: 25.00 | Train Loss: 4.774 | Train Acc: 11.33% | Val. Loss: 4.659 | Val. Acc: 12.87% | Test Loss: 4.659 | Test Acc: 12.87% |\n",
      "TEST ACCURACY: 12.99\n",
      "| Epoch: 26.00 | Train Loss: 4.762 | Train Acc: 11.44% | Val. Loss: 4.663 | Val. Acc: 12.94% | Test Loss: 4.663 | Test Acc: 12.94% |\n",
      "TEST ACCURACY: 12.99\n",
      "| Epoch: 27.00 | Train Loss: 4.762 | Train Acc: 11.58% | Val. Loss: 4.682 | Val. Acc: 12.26% | Test Loss: 4.682 | Test Acc: 12.26% |\n",
      "TEST ACCURACY: 12.99\n",
      "| Epoch: 28.00 | Train Loss: 4.769 | Train Acc: 11.32% | Val. Loss: 4.665 | Val. Acc: 12.59% | Test Loss: 4.665 | Test Acc: 12.59% |\n",
      "TEST ACCURACY: 12.99\n",
      "| Epoch: 29.00 | Train Loss: 4.755 | Train Acc: 11.54% | Val. Loss: 4.653 | Val. Acc: 12.76% | Test Loss: 4.653 | Test Acc: 12.76% |\n",
      "TEST ACCURACY: 12.99\n",
      "| Epoch: 30.00 | Train Loss: 4.746 | Train Acc: 11.92% | Val. Loss: 4.639 | Val. Acc: 13.01% | Test Loss: 4.639 | Test Acc: 13.01% |\n",
      "TEST ACCURACY: 13.01\n",
      "| Epoch: 31.00 | Train Loss: 4.752 | Train Acc: 11.61% | Val. Loss: 4.637 | Val. Acc: 12.96% | Test Loss: 4.637 | Test Acc: 12.96% |\n",
      "TEST ACCURACY: 13.01\n",
      "| Epoch: 32.00 | Train Loss: 4.744 | Train Acc: 11.89% | Val. Loss: 4.635 | Val. Acc: 12.91% | Test Loss: 4.635 | Test Acc: 12.91% |\n",
      "TEST ACCURACY: 13.01\n",
      "| Epoch: 33.00 | Train Loss: 4.743 | Train Acc: 11.78% | Val. Loss: 4.673 | Val. Acc: 11.58% | Test Loss: 4.673 | Test Acc: 11.58% |\n",
      "TEST ACCURACY: 13.01\n",
      "| Epoch: 34.00 | Train Loss: 4.735 | Train Acc: 11.88% | Val. Loss: 4.641 | Val. Acc: 12.73% | Test Loss: 4.641 | Test Acc: 12.73% |\n",
      "TEST ACCURACY: 13.01\n",
      "| Epoch: 35.00 | Train Loss: 4.735 | Train Acc: 11.87% | Val. Loss: 4.626 | Val. Acc: 13.16% | Test Loss: 4.626 | Test Acc: 13.16% |\n",
      "TEST ACCURACY: 13.16\n",
      "| Epoch: 36.00 | Train Loss: 4.734 | Train Acc: 11.76% | Val. Loss: 4.630 | Val. Acc: 13.04% | Test Loss: 4.630 | Test Acc: 13.04% |\n",
      "TEST ACCURACY: 13.16\n",
      "| Epoch: 37.00 | Train Loss: 4.757 | Train Acc: 11.58% | Val. Loss: 4.623 | Val. Acc: 13.05% | Test Loss: 4.623 | Test Acc: 13.05% |\n",
      "TEST ACCURACY: 13.16\n",
      "| Epoch: 38.00 | Train Loss: 4.723 | Train Acc: 12.04% | Val. Loss: 4.626 | Val. Acc: 13.04% | Test Loss: 4.626 | Test Acc: 13.04% |\n",
      "TEST ACCURACY: 13.16\n",
      "| Epoch: 39.00 | Train Loss: 4.730 | Train Acc: 11.86% | Val. Loss: 4.625 | Val. Acc: 12.87% | Test Loss: 4.625 | Test Acc: 12.87% |\n",
      "TEST ACCURACY: 13.16\n",
      "| Epoch: 40.00 | Train Loss: 4.740 | Train Acc: 11.87% | Val. Loss: 4.651 | Val. Acc: 12.28% | Test Loss: 4.651 | Test Acc: 12.28% |\n",
      "TEST ACCURACY: 13.16\n",
      "| Epoch: 41.00 | Train Loss: 4.723 | Train Acc: 11.99% | Val. Loss: 4.617 | Val. Acc: 13.19% | Test Loss: 4.617 | Test Acc: 13.19% |\n",
      "TEST ACCURACY: 13.19\n",
      "| Epoch: 42.00 | Train Loss: 4.714 | Train Acc: 12.23% | Val. Loss: 4.617 | Val. Acc: 13.26% | Test Loss: 4.617 | Test Acc: 13.26% |\n",
      "TEST ACCURACY: 13.26\n",
      "| Epoch: 43.00 | Train Loss: 4.715 | Train Acc: 12.15% | Val. Loss: 4.613 | Val. Acc: 13.30% | Test Loss: 4.613 | Test Acc: 13.30% |\n",
      "TEST ACCURACY: 13.3\n",
      "| Epoch: 44.00 | Train Loss: 4.718 | Train Acc: 12.01% | Val. Loss: 4.633 | Val. Acc: 12.77% | Test Loss: 4.633 | Test Acc: 12.77% |\n",
      "TEST ACCURACY: 13.3\n",
      "| Epoch: 45.00 | Train Loss: 4.708 | Train Acc: 12.22% | Val. Loss: 4.606 | Val. Acc: 13.39% | Test Loss: 4.606 | Test Acc: 13.39% |\n",
      "TEST ACCURACY: 13.39\n",
      "| Epoch: 46.00 | Train Loss: 4.707 | Train Acc: 12.26% | Val. Loss: 4.625 | Val. Acc: 13.13% | Test Loss: 4.625 | Test Acc: 13.13% |\n",
      "TEST ACCURACY: 13.39\n",
      "| Epoch: 47.00 | Train Loss: 4.721 | Train Acc: 12.08% | Val. Loss: 4.610 | Val. Acc: 13.28% | Test Loss: 4.610 | Test Acc: 13.28% |\n",
      "TEST ACCURACY: 13.39\n",
      "| Epoch: 48.00 | Train Loss: 4.701 | Train Acc: 12.37% | Val. Loss: 4.604 | Val. Acc: 13.28% | Test Loss: 4.604 | Test Acc: 13.28% |\n",
      "TEST ACCURACY: 13.39\n",
      "| Epoch: 49.00 | Train Loss: 4.709 | Train Acc: 12.18% | Val. Loss: 4.603 | Val. Acc: 13.35% | Test Loss: 4.603 | Test Acc: 13.35% |\n",
      "TEST ACCURACY: 13.39\n",
      "| Epoch: 50.00 | Train Loss: 4.702 | Train Acc: 12.30% | Val. Loss: 4.608 | Val. Acc: 12.93% | Test Loss: 4.608 | Test Acc: 12.93% |\n",
      "TEST ACCURACY: 13.39\n",
      "| Epoch: 51.00 | Train Loss: 4.697 | Train Acc: 12.34% | Val. Loss: 4.599 | Val. Acc: 13.32% | Test Loss: 4.599 | Test Acc: 13.32% |\n",
      "TEST ACCURACY: 13.39\n",
      "| Epoch: 52.00 | Train Loss: 4.695 | Train Acc: 12.46% | Val. Loss: 4.607 | Val. Acc: 13.25% | Test Loss: 4.607 | Test Acc: 13.25% |\n",
      "TEST ACCURACY: 13.39\n",
      "| Epoch: 53.00 | Train Loss: 4.694 | Train Acc: 12.28% | Val. Loss: 4.600 | Val. Acc: 13.35% | Test Loss: 4.600 | Test Acc: 13.35% |\n",
      "TEST ACCURACY: 13.39\n",
      "| Epoch: 54.00 | Train Loss: 4.695 | Train Acc: 12.41% | Val. Loss: 4.596 | Val. Acc: 13.50% | Test Loss: 4.596 | Test Acc: 13.50% |\n",
      "TEST ACCURACY: 13.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 55.00 | Train Loss: 4.687 | Train Acc: 12.59% | Val. Loss: 4.592 | Val. Acc: 13.48% | Test Loss: 4.592 | Test Acc: 13.48% |\n",
      "TEST ACCURACY: 13.5\n",
      "| Epoch: 56.00 | Train Loss: 4.690 | Train Acc: 12.34% | Val. Loss: 4.592 | Val. Acc: 13.34% | Test Loss: 4.592 | Test Acc: 13.34% |\n",
      "TEST ACCURACY: 13.5\n",
      "| Epoch: 57.00 | Train Loss: 4.686 | Train Acc: 12.62% | Val. Loss: 4.598 | Val. Acc: 13.33% | Test Loss: 4.598 | Test Acc: 13.33% |\n",
      "TEST ACCURACY: 13.5\n",
      "| Epoch: 58.00 | Train Loss: 4.685 | Train Acc: 12.52% | Val. Loss: 4.591 | Val. Acc: 13.44% | Test Loss: 4.591 | Test Acc: 13.44% |\n",
      "TEST ACCURACY: 13.5\n",
      "| Epoch: 59.00 | Train Loss: 4.681 | Train Acc: 12.48% | Val. Loss: 4.597 | Val. Acc: 12.94% | Test Loss: 4.597 | Test Acc: 12.94% |\n",
      "TEST ACCURACY: 13.5\n",
      "| Epoch: 60.00 | Train Loss: 4.680 | Train Acc: 12.49% | Val. Loss: 4.590 | Val. Acc: 13.41% | Test Loss: 4.590 | Test Acc: 13.41% |\n",
      "TEST ACCURACY: 13.5\n",
      "| Epoch: 61.00 | Train Loss: 4.681 | Train Acc: 12.59% | Val. Loss: 4.594 | Val. Acc: 13.34% | Test Loss: 4.594 | Test Acc: 13.34% |\n",
      "TEST ACCURACY: 13.5\n",
      "| Epoch: 62.00 | Train Loss: 4.679 | Train Acc: 12.53% | Val. Loss: 4.589 | Val. Acc: 13.55% | Test Loss: 4.589 | Test Acc: 13.55% |\n",
      "TEST ACCURACY: 13.55\n",
      "| Epoch: 63.00 | Train Loss: 4.683 | Train Acc: 12.65% | Val. Loss: 4.587 | Val. Acc: 13.31% | Test Loss: 4.587 | Test Acc: 13.31% |\n",
      "TEST ACCURACY: 13.55\n",
      "| Epoch: 64.00 | Train Loss: 4.681 | Train Acc: 12.53% | Val. Loss: 4.585 | Val. Acc: 13.35% | Test Loss: 4.585 | Test Acc: 13.35% |\n",
      "TEST ACCURACY: 13.55\n",
      "| Epoch: 65.00 | Train Loss: 4.677 | Train Acc: 12.61% | Val. Loss: 4.584 | Val. Acc: 13.41% | Test Loss: 4.584 | Test Acc: 13.41% |\n",
      "TEST ACCURACY: 13.55\n",
      "| Epoch: 66.00 | Train Loss: 4.674 | Train Acc: 12.74% | Val. Loss: 4.583 | Val. Acc: 13.48% | Test Loss: 4.583 | Test Acc: 13.48% |\n",
      "TEST ACCURACY: 13.55\n",
      "| Epoch: 67.00 | Train Loss: 4.679 | Train Acc: 12.65% | Val. Loss: 4.602 | Val. Acc: 12.79% | Test Loss: 4.602 | Test Acc: 12.79% |\n",
      "TEST ACCURACY: 13.55\n",
      "| Epoch: 68.00 | Train Loss: 4.697 | Train Acc: 12.25% | Val. Loss: 4.598 | Val. Acc: 13.27% | Test Loss: 4.598 | Test Acc: 13.27% |\n",
      "TEST ACCURACY: 13.55\n",
      "| Epoch: 69.00 | Train Loss: 4.673 | Train Acc: 12.79% | Val. Loss: 4.583 | Val. Acc: 13.56% | Test Loss: 4.583 | Test Acc: 13.56% |\n",
      "TEST ACCURACY: 13.56\n",
      "| Epoch: 70.00 | Train Loss: 4.674 | Train Acc: 12.69% | Val. Loss: 4.585 | Val. Acc: 13.38% | Test Loss: 4.585 | Test Acc: 13.38% |\n",
      "TEST ACCURACY: 13.56\n",
      "| Epoch: 71.00 | Train Loss: 4.670 | Train Acc: 12.64% | Val. Loss: 4.584 | Val. Acc: 13.49% | Test Loss: 4.584 | Test Acc: 13.49% |\n",
      "TEST ACCURACY: 13.56\n",
      "| Epoch: 72.00 | Train Loss: 4.673 | Train Acc: 12.61% | Val. Loss: 4.582 | Val. Acc: 13.50% | Test Loss: 4.582 | Test Acc: 13.50% |\n",
      "TEST ACCURACY: 13.56\n",
      "| Epoch: 73.00 | Train Loss: 4.673 | Train Acc: 12.65% | Val. Loss: 4.585 | Val. Acc: 13.56% | Test Loss: 4.585 | Test Acc: 13.56% |\n",
      "TEST ACCURACY: 13.56\n",
      "| Epoch: 74.00 | Train Loss: 4.671 | Train Acc: 12.73% | Val. Loss: 4.590 | Val. Acc: 13.20% | Test Loss: 4.590 | Test Acc: 13.20% |\n",
      "TEST ACCURACY: 13.56\n",
      "| Epoch: 75.00 | Train Loss: 4.668 | Train Acc: 12.76% | Val. Loss: 4.580 | Val. Acc: 13.60% | Test Loss: 4.580 | Test Acc: 13.60% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 76.00 | Train Loss: 4.667 | Train Acc: 12.72% | Val. Loss: 4.581 | Val. Acc: 13.47% | Test Loss: 4.581 | Test Acc: 13.47% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 77.00 | Train Loss: 4.686 | Train Acc: 12.46% | Val. Loss: 4.759 | Val. Acc: 10.22% | Test Loss: 4.759 | Test Acc: 10.22% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 78.00 | Train Loss: 4.671 | Train Acc: 12.85% | Val. Loss: 4.580 | Val. Acc: 13.40% | Test Loss: 4.580 | Test Acc: 13.40% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 79.00 | Train Loss: 4.664 | Train Acc: 12.87% | Val. Loss: 4.579 | Val. Acc: 13.44% | Test Loss: 4.579 | Test Acc: 13.44% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 80.00 | Train Loss: 4.677 | Train Acc: 12.67% | Val. Loss: 4.579 | Val. Acc: 13.47% | Test Loss: 4.579 | Test Acc: 13.47% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 81.00 | Train Loss: 4.667 | Train Acc: 12.77% | Val. Loss: 4.578 | Val. Acc: 13.48% | Test Loss: 4.578 | Test Acc: 13.48% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 82.00 | Train Loss: 4.666 | Train Acc: 12.55% | Val. Loss: 4.577 | Val. Acc: 13.58% | Test Loss: 4.577 | Test Acc: 13.58% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 83.00 | Train Loss: 4.663 | Train Acc: 12.71% | Val. Loss: 4.577 | Val. Acc: 13.40% | Test Loss: 4.577 | Test Acc: 13.40% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 84.00 | Train Loss: 4.663 | Train Acc: 12.80% | Val. Loss: 4.576 | Val. Acc: 13.56% | Test Loss: 4.576 | Test Acc: 13.56% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 85.00 | Train Loss: 4.662 | Train Acc: 12.72% | Val. Loss: 4.576 | Val. Acc: 13.57% | Test Loss: 4.576 | Test Acc: 13.57% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 86.00 | Train Loss: 4.658 | Train Acc: 12.93% | Val. Loss: 4.576 | Val. Acc: 13.51% | Test Loss: 4.576 | Test Acc: 13.51% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 87.00 | Train Loss: 4.661 | Train Acc: 13.01% | Val. Loss: 4.580 | Val. Acc: 13.35% | Test Loss: 4.580 | Test Acc: 13.35% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 88.00 | Train Loss: 4.780 | Train Acc: 11.91% | Val. Loss: 4.578 | Val. Acc: 13.47% | Test Loss: 4.578 | Test Acc: 13.47% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 89.00 | Train Loss: 4.667 | Train Acc: 12.79% | Val. Loss: 4.576 | Val. Acc: 13.56% | Test Loss: 4.576 | Test Acc: 13.56% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 90.00 | Train Loss: 4.665 | Train Acc: 12.63% | Val. Loss: 4.575 | Val. Acc: 13.59% | Test Loss: 4.575 | Test Acc: 13.59% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 91.00 | Train Loss: 4.659 | Train Acc: 12.81% | Val. Loss: 4.574 | Val. Acc: 13.50% | Test Loss: 4.574 | Test Acc: 13.50% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 92.00 | Train Loss: 4.662 | Train Acc: 12.86% | Val. Loss: 4.575 | Val. Acc: 13.53% | Test Loss: 4.575 | Test Acc: 13.53% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 93.00 | Train Loss: 4.657 | Train Acc: 13.01% | Val. Loss: 4.574 | Val. Acc: 13.53% | Test Loss: 4.574 | Test Acc: 13.53% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 94.00 | Train Loss: 4.660 | Train Acc: 12.81% | Val. Loss: 4.574 | Val. Acc: 13.53% | Test Loss: 4.574 | Test Acc: 13.53% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 95.00 | Train Loss: 4.673 | Train Acc: 12.70% | Val. Loss: 4.577 | Val. Acc: 13.49% | Test Loss: 4.577 | Test Acc: 13.49% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 96.00 | Train Loss: 4.659 | Train Acc: 12.75% | Val. Loss: 4.572 | Val. Acc: 13.59% | Test Loss: 4.572 | Test Acc: 13.59% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 97.00 | Train Loss: 4.655 | Train Acc: 12.77% | Val. Loss: 4.572 | Val. Acc: 13.57% | Test Loss: 4.572 | Test Acc: 13.57% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 98.00 | Train Loss: 4.657 | Train Acc: 12.87% | Val. Loss: 4.572 | Val. Acc: 13.58% | Test Loss: 4.572 | Test Acc: 13.58% |\n",
      "TEST ACCURACY: 13.6\n",
      "| Epoch: 99.00 | Train Loss: 4.654 | Train Acc: 13.02% | Val. Loss: 4.572 | Val. Acc: 13.65% | Test Loss: 4.572 | Test Acc: 13.65% |\n",
      "TEST ACCURACY: 13.65\n",
      "| Epoch: 100.00 | Train Loss: 4.655 | Train Acc: 12.85% | Val. Loss: 4.572 | Val. Acc: 13.54% | Test Loss: 4.572 | Test Acc: 13.54% |\n",
      "TEST ACCURACY: 13.65\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    log['train_loss'].append(train_loss)\n",
    "    log['test_loss'].append(test_loss)\n",
    "    log['train_acc'].append(train_acc)\n",
    "    log['test_acc'].append(test_acc)\n",
    "    log['valid_acc'].append(valid_acc)\n",
    "    log['valid_loss'].append(valid_loss)\n",
    "\n",
    "    if best_result[\"valid_acc\"] < valid_acc:\n",
    "        best_result[\"epoch\"] = epoch\n",
    "        best_result[\"train_acc\"] = train_acc\n",
    "        best_result[\"valid_acc\"] = valid_acc\n",
    "        best_result[\"test_acc\"] = test_acc\n",
    "\n",
    "    #if args.fout is not None:\n",
    "    #    with open(args.fout+f\"{model_name}-best.pkl\", 'wb') as f:\n",
    "    #        pickle.dump(best_result, f)\n",
    "    print(f'| Epoch: {epoch+1:.2f} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% | Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')\n",
    "    print (\"TEST ACCURACY:\", np.round(best_result[\"test_acc\"] * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compression_rate': 7.936507936507937,\n",
       " 'train_loss': [6.173078198089599,\n",
       "  5.529343283691406,\n",
       "  5.325558062438965,\n",
       "  5.212327967681885,\n",
       "  5.135382705078125,\n",
       "  5.045652011260986,\n",
       "  5.006695288238525,\n",
       "  4.971650762863159,\n",
       "  4.957384234924317,\n",
       "  4.938258407592773,\n",
       "  4.899241761779785,\n",
       "  4.881121518249512,\n",
       "  4.86749458114624,\n",
       "  4.853388886108398,\n",
       "  4.8375276414489745,\n",
       "  4.832554296722412,\n",
       "  4.82023506362915,\n",
       "  4.80969801864624,\n",
       "  4.804461853027344,\n",
       "  4.810269001159668,\n",
       "  4.788954376983643,\n",
       "  4.782254061584473,\n",
       "  4.77753087600708,\n",
       "  4.776470971069336,\n",
       "  4.774167734222412,\n",
       "  4.7621871069335935,\n",
       "  4.76187069152832,\n",
       "  4.768544646759033,\n",
       "  4.754573500671387,\n",
       "  4.7456744482421875,\n",
       "  4.752347720642089,\n",
       "  4.743860781402588,\n",
       "  4.742851383666992,\n",
       "  4.7347481719970705,\n",
       "  4.735160696411133,\n",
       "  4.734422542572021,\n",
       "  4.756992329101562,\n",
       "  4.722698518066406,\n",
       "  4.729533582992554,\n",
       "  4.739586484375,\n",
       "  4.723484573974609,\n",
       "  4.713653395080566,\n",
       "  4.715449489746094,\n",
       "  4.717872949676513,\n",
       "  4.70818833480835,\n",
       "  4.706871774978637,\n",
       "  4.720720984344482,\n",
       "  4.700875154724121,\n",
       "  4.708516979675293,\n",
       "  4.702422365264892,\n",
       "  4.69743359588623,\n",
       "  4.695111466522217,\n",
       "  4.6944614067840575,\n",
       "  4.695278968811035,\n",
       "  4.686823921051025,\n",
       "  4.6897164561462406,\n",
       "  4.686319821777344,\n",
       "  4.685178183441162,\n",
       "  4.680660970611572,\n",
       "  4.680301649475098,\n",
       "  4.680533546142578,\n",
       "  4.679036688537598,\n",
       "  4.683151891326904,\n",
       "  4.6810446057128905,\n",
       "  4.676868741607666,\n",
       "  4.674239976043701,\n",
       "  4.6789791366577145,\n",
       "  4.6965587854003905,\n",
       "  4.672851039276123,\n",
       "  4.673807255249024,\n",
       "  4.670141492614746,\n",
       "  4.673452108612061,\n",
       "  4.673112197875977,\n",
       "  4.670872651367188,\n",
       "  4.668145059967041,\n",
       "  4.667204581604004,\n",
       "  4.685772679748535,\n",
       "  4.6711573825073245,\n",
       "  4.663952374572754,\n",
       "  4.67704217453003,\n",
       "  4.666563614349365,\n",
       "  4.6663080627441405,\n",
       "  4.662874234008789,\n",
       "  4.663021224517823,\n",
       "  4.661597176055908,\n",
       "  4.65770240600586,\n",
       "  4.660692796325684,\n",
       "  4.780081296234131,\n",
       "  4.667226144866944,\n",
       "  4.665233537597656,\n",
       "  4.6589626068115235,\n",
       "  4.661657339172363,\n",
       "  4.657386958847046,\n",
       "  4.659803662567139,\n",
       "  4.67320146560669,\n",
       "  4.659039603271484,\n",
       "  4.655248399810791,\n",
       "  4.6568416708374025,\n",
       "  4.653784306945801,\n",
       "  4.655492253952026],\n",
       " 'test_loss': [5.589917859649658,\n",
       "  5.280421542663574,\n",
       "  5.117500668334961,\n",
       "  5.067100628967285,\n",
       "  4.960946738128662,\n",
       "  4.993707827148437,\n",
       "  4.858245442962646,\n",
       "  4.84057100479126,\n",
       "  4.841321787033081,\n",
       "  4.792793811645508,\n",
       "  4.780743734741211,\n",
       "  4.758024530105591,\n",
       "  4.750944653167725,\n",
       "  4.72914373626709,\n",
       "  4.710567642974853,\n",
       "  4.705126943130493,\n",
       "  4.714713887634277,\n",
       "  4.689351462783813,\n",
       "  4.710191491775513,\n",
       "  4.678688612594605,\n",
       "  4.6851048513793945,\n",
       "  4.666324335632324,\n",
       "  4.668412182388305,\n",
       "  4.659812887153626,\n",
       "  4.659243436813354,\n",
       "  4.662535518722534,\n",
       "  4.681988516616821,\n",
       "  4.665374003219604,\n",
       "  4.653160883255005,\n",
       "  4.638865805168152,\n",
       "  4.637435637512207,\n",
       "  4.635223078041077,\n",
       "  4.673020285263061,\n",
       "  4.64091753364563,\n",
       "  4.62613047088623,\n",
       "  4.630430251083374,\n",
       "  4.623486123886108,\n",
       "  4.625906320915222,\n",
       "  4.624502388687134,\n",
       "  4.650673118400574,\n",
       "  4.616929789810181,\n",
       "  4.617401670036316,\n",
       "  4.612838267250061,\n",
       "  4.63316407447815,\n",
       "  4.605623835868835,\n",
       "  4.625303219642639,\n",
       "  4.610216433067322,\n",
       "  4.604457241249085,\n",
       "  4.6028380562210085,\n",
       "  4.608106020088195,\n",
       "  4.598590004501343,\n",
       "  4.606731813087463,\n",
       "  4.599559798622131,\n",
       "  4.595565254478455,\n",
       "  4.592448597984314,\n",
       "  4.592279067993164,\n",
       "  4.5981779019927975,\n",
       "  4.591011828079224,\n",
       "  4.597388817138672,\n",
       "  4.5898178723526,\n",
       "  4.593652379493713,\n",
       "  4.588562349624634,\n",
       "  4.58735618888855,\n",
       "  4.585165709152221,\n",
       "  4.58425563659668,\n",
       "  4.5828928133392335,\n",
       "  4.601684097900391,\n",
       "  4.597816089553833,\n",
       "  4.582695458412171,\n",
       "  4.585341837959289,\n",
       "  4.583649244575501,\n",
       "  4.582214761085511,\n",
       "  4.585126908340454,\n",
       "  4.589881324615479,\n",
       "  4.58024134262085,\n",
       "  4.581217077217102,\n",
       "  4.758685950927735,\n",
       "  4.579857399177551,\n",
       "  4.5793295333480835,\n",
       "  4.578536109237671,\n",
       "  4.577533590774536,\n",
       "  4.576812019882202,\n",
       "  4.576648755340576,\n",
       "  4.575765142364502,\n",
       "  4.576010471801758,\n",
       "  4.575933131828308,\n",
       "  4.5802729101181034,\n",
       "  4.578238899612427,\n",
       "  4.576144656372071,\n",
       "  4.574788189544678,\n",
       "  4.574005779533386,\n",
       "  4.574854199790955,\n",
       "  4.573979776992798,\n",
       "  4.573918477897644,\n",
       "  4.576565017204285,\n",
       "  4.571617905387878,\n",
       "  4.572397921943664,\n",
       "  4.572140788269043,\n",
       "  4.5718355966568,\n",
       "  4.571925772323608],\n",
       " 'valid_loss': [5.589917859649658,\n",
       "  5.280421542663574,\n",
       "  5.117500668334961,\n",
       "  5.067100628967285,\n",
       "  4.960946738128662,\n",
       "  4.993707827148437,\n",
       "  4.858245442962646,\n",
       "  4.84057100479126,\n",
       "  4.841321787033081,\n",
       "  4.792793811645508,\n",
       "  4.780743734741211,\n",
       "  4.758024530105591,\n",
       "  4.750944653167725,\n",
       "  4.72914373626709,\n",
       "  4.710567642974853,\n",
       "  4.705126943130493,\n",
       "  4.714713887634277,\n",
       "  4.689351462783813,\n",
       "  4.710191491775513,\n",
       "  4.678688612594605,\n",
       "  4.6851048513793945,\n",
       "  4.666324335632324,\n",
       "  4.668412182388305,\n",
       "  4.659812887153626,\n",
       "  4.659243436813354,\n",
       "  4.662535518722534,\n",
       "  4.681988516616821,\n",
       "  4.665374003219604,\n",
       "  4.653160883255005,\n",
       "  4.638865805168152,\n",
       "  4.637435637512207,\n",
       "  4.635223078041077,\n",
       "  4.673020285263061,\n",
       "  4.64091753364563,\n",
       "  4.62613047088623,\n",
       "  4.630430251083374,\n",
       "  4.623486123886108,\n",
       "  4.625906320915222,\n",
       "  4.624502388687134,\n",
       "  4.650673118400574,\n",
       "  4.616929789810181,\n",
       "  4.617401670036316,\n",
       "  4.612838267250061,\n",
       "  4.63316407447815,\n",
       "  4.605623835868835,\n",
       "  4.625303219642639,\n",
       "  4.610216433067322,\n",
       "  4.604457241249085,\n",
       "  4.6028380562210085,\n",
       "  4.608106020088195,\n",
       "  4.598590004501343,\n",
       "  4.606731813087463,\n",
       "  4.599559798622131,\n",
       "  4.595565254478455,\n",
       "  4.592448597984314,\n",
       "  4.592279067993164,\n",
       "  4.5981779019927975,\n",
       "  4.591011828079224,\n",
       "  4.597388817138672,\n",
       "  4.5898178723526,\n",
       "  4.593652379493713,\n",
       "  4.588562349624634,\n",
       "  4.58735618888855,\n",
       "  4.585165709152221,\n",
       "  4.58425563659668,\n",
       "  4.5828928133392335,\n",
       "  4.601684097900391,\n",
       "  4.597816089553833,\n",
       "  4.582695458412171,\n",
       "  4.585341837959289,\n",
       "  4.583649244575501,\n",
       "  4.582214761085511,\n",
       "  4.585126908340454,\n",
       "  4.589881324615479,\n",
       "  4.58024134262085,\n",
       "  4.581217077217102,\n",
       "  4.758685950927735,\n",
       "  4.579857399177551,\n",
       "  4.5793295333480835,\n",
       "  4.578536109237671,\n",
       "  4.577533590774536,\n",
       "  4.576812019882202,\n",
       "  4.576648755340576,\n",
       "  4.575765142364502,\n",
       "  4.576010471801758,\n",
       "  4.575933131828308,\n",
       "  4.5802729101181034,\n",
       "  4.578238899612427,\n",
       "  4.576144656372071,\n",
       "  4.574788189544678,\n",
       "  4.574005779533386,\n",
       "  4.574854199790955,\n",
       "  4.573979776992798,\n",
       "  4.573918477897644,\n",
       "  4.576565017204285,\n",
       "  4.571617905387878,\n",
       "  4.572397921943664,\n",
       "  4.572140788269043,\n",
       "  4.5718355966568,\n",
       "  4.571925772323608],\n",
       " 'train_acc': [0.01604,\n",
       "  0.03424,\n",
       "  0.04352,\n",
       "  0.05214,\n",
       "  0.0591,\n",
       "  0.06742,\n",
       "  0.07396,\n",
       "  0.07906,\n",
       "  0.08292,\n",
       "  0.08704,\n",
       "  0.09186,\n",
       "  0.09498,\n",
       "  0.09982,\n",
       "  0.102,\n",
       "  0.10306,\n",
       "  0.10446,\n",
       "  0.10584,\n",
       "  0.10812,\n",
       "  0.1093,\n",
       "  0.10828,\n",
       "  0.1115,\n",
       "  0.11408,\n",
       "  0.11516,\n",
       "  0.11352,\n",
       "  0.11328,\n",
       "  0.11436,\n",
       "  0.11582,\n",
       "  0.11324,\n",
       "  0.11538,\n",
       "  0.1192,\n",
       "  0.11608,\n",
       "  0.11894,\n",
       "  0.11784,\n",
       "  0.11876,\n",
       "  0.11866,\n",
       "  0.11764,\n",
       "  0.11582,\n",
       "  0.12042,\n",
       "  0.11858,\n",
       "  0.11872,\n",
       "  0.11994,\n",
       "  0.1223,\n",
       "  0.12148,\n",
       "  0.12006,\n",
       "  0.1222,\n",
       "  0.12262,\n",
       "  0.1208,\n",
       "  0.12372,\n",
       "  0.12178,\n",
       "  0.123,\n",
       "  0.1234,\n",
       "  0.12462,\n",
       "  0.12284,\n",
       "  0.12406,\n",
       "  0.12588,\n",
       "  0.12342,\n",
       "  0.12618,\n",
       "  0.12524,\n",
       "  0.12482,\n",
       "  0.12494,\n",
       "  0.12588,\n",
       "  0.12532,\n",
       "  0.12648,\n",
       "  0.1253,\n",
       "  0.12608,\n",
       "  0.12742,\n",
       "  0.12648,\n",
       "  0.1225,\n",
       "  0.12794,\n",
       "  0.12692,\n",
       "  0.12638,\n",
       "  0.12612,\n",
       "  0.12652,\n",
       "  0.12726,\n",
       "  0.1276,\n",
       "  0.12716,\n",
       "  0.12462,\n",
       "  0.1285,\n",
       "  0.12866,\n",
       "  0.12674,\n",
       "  0.12774,\n",
       "  0.12554,\n",
       "  0.12712,\n",
       "  0.12796,\n",
       "  0.12722,\n",
       "  0.12928,\n",
       "  0.13008,\n",
       "  0.11908,\n",
       "  0.12786,\n",
       "  0.12632,\n",
       "  0.12808,\n",
       "  0.12862,\n",
       "  0.13014,\n",
       "  0.12814,\n",
       "  0.12698,\n",
       "  0.1275,\n",
       "  0.12768,\n",
       "  0.12872,\n",
       "  0.13016,\n",
       "  0.12848],\n",
       " 'test_acc': [0.0321,\n",
       "  0.04736,\n",
       "  0.06322,\n",
       "  0.06664,\n",
       "  0.07674,\n",
       "  0.06472,\n",
       "  0.0936,\n",
       "  0.0983,\n",
       "  0.09908,\n",
       "  0.10634,\n",
       "  0.10584,\n",
       "  0.1156,\n",
       "  0.10552,\n",
       "  0.11674,\n",
       "  0.11784,\n",
       "  0.12124,\n",
       "  0.11538,\n",
       "  0.12188,\n",
       "  0.11332,\n",
       "  0.12586,\n",
       "  0.1231,\n",
       "  0.12988,\n",
       "  0.12858,\n",
       "  0.12888,\n",
       "  0.12874,\n",
       "  0.12942,\n",
       "  0.12256,\n",
       "  0.12592,\n",
       "  0.12764,\n",
       "  0.13012,\n",
       "  0.12962,\n",
       "  0.1291,\n",
       "  0.11584,\n",
       "  0.12734,\n",
       "  0.1316,\n",
       "  0.13044,\n",
       "  0.13052,\n",
       "  0.13038,\n",
       "  0.12868,\n",
       "  0.12282,\n",
       "  0.13192,\n",
       "  0.1326,\n",
       "  0.13296,\n",
       "  0.12772,\n",
       "  0.13386,\n",
       "  0.13126,\n",
       "  0.1328,\n",
       "  0.13278,\n",
       "  0.13348,\n",
       "  0.12934,\n",
       "  0.13316,\n",
       "  0.13248,\n",
       "  0.1335,\n",
       "  0.13496,\n",
       "  0.13478,\n",
       "  0.13342,\n",
       "  0.1333,\n",
       "  0.13442,\n",
       "  0.12944,\n",
       "  0.13412,\n",
       "  0.1334,\n",
       "  0.13548,\n",
       "  0.13308,\n",
       "  0.13348,\n",
       "  0.13408,\n",
       "  0.13482,\n",
       "  0.12786,\n",
       "  0.13272,\n",
       "  0.13564,\n",
       "  0.1338,\n",
       "  0.13492,\n",
       "  0.13498,\n",
       "  0.13564,\n",
       "  0.13196,\n",
       "  0.136,\n",
       "  0.13474,\n",
       "  0.10218,\n",
       "  0.13398,\n",
       "  0.13444,\n",
       "  0.1347,\n",
       "  0.13476,\n",
       "  0.13578,\n",
       "  0.13402,\n",
       "  0.1356,\n",
       "  0.13574,\n",
       "  0.13506,\n",
       "  0.13348,\n",
       "  0.13472,\n",
       "  0.1356,\n",
       "  0.13594,\n",
       "  0.135,\n",
       "  0.13528,\n",
       "  0.13534,\n",
       "  0.13534,\n",
       "  0.13492,\n",
       "  0.1359,\n",
       "  0.1357,\n",
       "  0.13578,\n",
       "  0.13646,\n",
       "  0.1354],\n",
       " 'valid_acc': [0.0321,\n",
       "  0.04736,\n",
       "  0.06322,\n",
       "  0.06664,\n",
       "  0.07674,\n",
       "  0.06472,\n",
       "  0.0936,\n",
       "  0.0983,\n",
       "  0.09908,\n",
       "  0.10634,\n",
       "  0.10584,\n",
       "  0.1156,\n",
       "  0.10552,\n",
       "  0.11674,\n",
       "  0.11784,\n",
       "  0.12124,\n",
       "  0.11538,\n",
       "  0.12188,\n",
       "  0.11332,\n",
       "  0.12586,\n",
       "  0.1231,\n",
       "  0.12988,\n",
       "  0.12858,\n",
       "  0.12888,\n",
       "  0.12874,\n",
       "  0.12942,\n",
       "  0.12256,\n",
       "  0.12592,\n",
       "  0.12764,\n",
       "  0.13012,\n",
       "  0.12962,\n",
       "  0.1291,\n",
       "  0.11584,\n",
       "  0.12734,\n",
       "  0.1316,\n",
       "  0.13044,\n",
       "  0.13052,\n",
       "  0.13038,\n",
       "  0.12868,\n",
       "  0.12282,\n",
       "  0.13192,\n",
       "  0.1326,\n",
       "  0.13296,\n",
       "  0.12772,\n",
       "  0.13386,\n",
       "  0.13126,\n",
       "  0.1328,\n",
       "  0.13278,\n",
       "  0.13348,\n",
       "  0.12934,\n",
       "  0.13316,\n",
       "  0.13248,\n",
       "  0.1335,\n",
       "  0.13496,\n",
       "  0.13478,\n",
       "  0.13342,\n",
       "  0.1333,\n",
       "  0.13442,\n",
       "  0.12944,\n",
       "  0.13412,\n",
       "  0.1334,\n",
       "  0.13548,\n",
       "  0.13308,\n",
       "  0.13348,\n",
       "  0.13408,\n",
       "  0.13482,\n",
       "  0.12786,\n",
       "  0.13272,\n",
       "  0.13564,\n",
       "  0.1338,\n",
       "  0.13492,\n",
       "  0.13498,\n",
       "  0.13564,\n",
       "  0.13196,\n",
       "  0.136,\n",
       "  0.13474,\n",
       "  0.10218,\n",
       "  0.13398,\n",
       "  0.13444,\n",
       "  0.1347,\n",
       "  0.13476,\n",
       "  0.13578,\n",
       "  0.13402,\n",
       "  0.1356,\n",
       "  0.13574,\n",
       "  0.13506,\n",
       "  0.13348,\n",
       "  0.13472,\n",
       "  0.1356,\n",
       "  0.13594,\n",
       "  0.135,\n",
       "  0.13528,\n",
       "  0.13534,\n",
       "  0.13534,\n",
       "  0.13492,\n",
       "  0.1359,\n",
       "  0.1357,\n",
       "  0.13578,\n",
       "  0.13646,\n",
       "  0.1354]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 98, 'train_acc': 0.13016, 'valid_acc': 0.13646, 'test_acc': 0.13646}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
