{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "#print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--embedding', \n",
    "    default='tt',\n",
    "    choices=['tt', 'tr', 'full'],\n",
    "    type=str)\n",
    "parser.add_argument('--ranks', type=int, default=8)\n",
    "parser.add_argument('--d', type=int, default=3)\n",
    "parser.add_argument('--embed_dim', type=int, default=64)\n",
    "parser.add_argument('--voc_dim', default=250, type=int)\n",
    "parser.add_argument('--lr', default=5e-4)\n",
    "parser.add_argument('--gpu', default='', type=str)\n",
    "parser.add_argument('--hidden_dim', default=128, type=int)\n",
    "parser.add_argument('--n_epochs',  default=10, type=int)\n",
    "parser.add_argument('--fout',  default=\"logdir/\", type=str)\n",
    "parser.add_argument('--dropout', default=0.5, type=float)\n",
    "parser.add_argument(\n",
    "    '--dataset',\n",
    "    default='TB',\n",
    "    type=str)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embedding == 'tt':\n",
    "    tt = \"tt\"\n",
    "elif args.embedding == 'tr':\n",
    "    tt = 'tr'\n",
    "else:             \n",
    "    tt = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"{args.dataset}-dim_{args.embed_dim}-d_{args.d}-ranks_{args.ranks}-{tt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=args.gpu\n",
    "\n",
    "\n",
    "import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import t3nsor as t3\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.optim as optim\n",
    "from models import LSTM_Classifier\n",
    "import pickle\n",
    "import random\n",
    "import spacy\n",
    "from spacy.cli.download import download\n",
    "#download(model=\"en_core_web_sm\")\n",
    "random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#\n",
    "#spacy.blank(\"en\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', fix_length=1000)\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 1\n",
    "train_data, test_ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "test_list = list(test_)\n",
    "random.shuffle(test_list)\n",
    "test_data_ = test_list[:12500]\n",
    "val_data_ = test_list[12500:]\n",
    "valid_data = data.dataset.Dataset(val_data_, fields=[('text', TEXT), ('label', LABEL)])\n",
    "test_data = data.dataset.Dataset(test_data_, fields=[('text', TEXT), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(ex):\n",
    "    return len(ex.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=args.voc_dim - 2)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)\n",
    "\n",
    "valid_iterator.sort_key = sort_key\n",
    "test_iterator.sort_key = sort_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = args.embed_dim\n",
    "HIDDEN_DIM = args.hidden_dim\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = args.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vocab_size = len(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_Classifier(embedding_dim=EMBEDDING_DIM,\n",
    "                             hidden_dim=HIDDEN_DIM,\n",
    "                             output_dim=OUTPUT_DIM,\n",
    "                             n_layers=N_LAYERS,\n",
    "                             bidirectional=BIDIRECTIONAL,\n",
    "                             dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embedding == 'tt':\n",
    "        embed_model = t3.TTEmbedding(\n",
    "            voc_size=INPUT_DIM,\n",
    "            emb_size=EMBEDDING_DIM,\n",
    "            auto_shapes=True,\n",
    "            auto_shape_mode='mixed',\n",
    "            d=args.d,\n",
    "            tt_rank=args.ranks,\n",
    "            padding_idx=1\n",
    "        )\n",
    "        compression_rate = INPUT_DIM * EMBEDDING_DIM / embed_model.tt_matrix.dof\n",
    "elif args.embedding == 'tr':\n",
    "        embed_model = t3.TREmbedding(\n",
    "            voc_size=INPUT_DIM,\n",
    "            emb_size=EMBEDDING_DIM,\n",
    "            auto_shapes=True,\n",
    "            auto_shape_mode='mixed',\n",
    "            d=args.d,\n",
    "            tr_rank=args.ranks,\n",
    "            padding_idx=1\n",
    "        )\n",
    "        compression_rate = INPUT_DIM * EMBEDDING_DIM / embed_model.tr_matrix.dof\n",
    "else:\n",
    "    embed_model = nn.Embedding(\n",
    "        num_embeddings=INPUT_DIM,\n",
    "        embedding_dim=EMBEDDING_DIM\n",
    "    )\n",
    "    compression_rate = 1.0\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, target):\n",
    "    labels = target.type(torch.LongTensor).to(logits.device)\n",
    "    return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(embed_model, lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == 'imdb':\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    #criterion = criterion.to(device)\n",
    "elif args.dataset[:3] == 'sst':\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #criterion = criterion.to(device)\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "print(model)\n",
    "N_EPOCHS = args.n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {\n",
    "    'compression_rate':compression_rate,\n",
    "    'train_loss':[], 'test_loss':[], 'valid_loss':[],\n",
    "    'train_acc':[], 'test_acc':[], 'valid_acc':[]}\n",
    "best_result = {\n",
    "    \"epoch\": 0, \"train_acc\": 0, \"valid_acc\": 0, \"train_acc\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    if len(preds.shape) == 1:\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    else:\n",
    "        rounded_preds = preds.argmax(1)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        dtype = torch.LongTensor\n",
    "    elif isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        device = batch.text.device\n",
    "        labels = batch.label.type(dtype).to(device)\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        B = batch.label.shape[0]\n",
    "\n",
    "        epoch_loss += B * loss.item()\n",
    "        epoch_acc += B * acc.item()\n",
    "\n",
    "        total_len += B\n",
    "\n",
    "\n",
    "        if i > len(iterator):\n",
    "            break\n",
    "\n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        dtype = torch.LongTensor\n",
    "    elif isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            device = batch.text.device\n",
    "            labels = batch.label.type(dtype).to(device)\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            B = batch.label.shape[0]\n",
    "\n",
    "            epoch_loss += B * loss.item()\n",
    "            epoch_acc += B * acc.item()\n",
    "            total_len += B\n",
    "\n",
    "            if i > len(iterator):\n",
    "                break\n",
    "\n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    log['train_loss'].append(train_loss)\n",
    "    log['test_loss'].append(test_loss)\n",
    "    log['train_acc'].append(train_acc)\n",
    "    log['test_acc'].append(test_acc)\n",
    "    log['valid_acc'].append(valid_acc)\n",
    "    log['valid_loss'].append(valid_loss)\n",
    "\n",
    "    if best_result[\"valid_acc\"] < valid_acc:\n",
    "        best_result[\"epoch\"] = epoch\n",
    "        best_result[\"train_acc\"] = train_acc\n",
    "        best_result[\"valid_acc\"] = valid_acc\n",
    "        best_result[\"test_acc\"] = test_acc\n",
    "\n",
    "    #if args.fout is not None:\n",
    "    #    with open(args.fout+f\"{model_name}-best.pkl\", 'wb') as f:\n",
    "    #        pickle.dump(best_result, f)\n",
    "    print(f'| Epoch: {epoch+1:.2} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% | Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')\n",
    "    print (\"TEST ACCURACY:\", np.round(best_result[\"test_acc\"] * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
