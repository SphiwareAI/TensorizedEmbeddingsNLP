{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing for TB fasta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_file_location = os.path.join('/home/mashjunior/Downloads/', 'rif_resis_rpob', '*.fasta')\n",
    "#susc_file_location = os.path.join('/home/mashjunior/Downloads/', 'rif_susc_rpob', '*.fasta')\n",
    "#print(res_file_location)\n",
    "#print(susc_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import glob\n",
    "#res_filenames = glob.glob(res_file_location)\n",
    "#print(res_filenames)\n",
    "#susc_filenames = glob.glob(susc_file_location)\n",
    "#print(susc_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def convert(s):\n",
    "#  \n",
    "#    # initialization of string to \"\"\n",
    "#    new = \"\"\n",
    "  \n",
    "\n",
    "# traverse in the string \n",
    "#    for x in s:\n",
    "#        new += x \n",
    "#  \n",
    "    # return string \n",
    "#    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def build_kmers(sequence, ksize):\n",
    "#    kmers = []\n",
    "#    n_kmers = len(sequence) - ksize + 1\n",
    "\n",
    "#    for i in range(n_kmers):\n",
    "#        kmer = sequence[i:i + ksize]\n",
    "#        kmers.append(kmer)\n",
    "\n",
    "#    return kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_list = []\n",
    "#for f in res_filenames:\n",
    "#    rif_resis_rpob = open(f,'r')\n",
    "#    res_data = rif_resis_rpob.readlines()\n",
    "#    res_data = res_data[1:]\n",
    "#    res_data = [x.strip() for x in res_data]\n",
    "#    res_data = convert(res_data)\n",
    "#    res_data = ' '.join(build_kmers(res_data, 6))\n",
    "#    res_list.append(res_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#susc_list = []\n",
    "#for f in susc_filenames:\n",
    "#    rif_susc_rpob = open(f,'r')\n",
    "#    susc_data = rif_susc_rpob.readlines()\n",
    "#    susc_data = susc_data[1:]\n",
    "#    susc_data = [x.strip() for x in susc_data]\n",
    "#    susc_data = convert(susc_data)\n",
    "#    susc_data = ' '.join(build_kmers(susc_data, 6))\n",
    "#    susc_list.append(susc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a_list = [\"abc\", \"def\", \"ghi\"]\n",
    "#textfile = open(\"TB_contig.tsv\", \"w\")\n",
    "#textfile.write('TEXT' + ',' + 'LABEL' + \"\\n\")\n",
    "#for res, susc in zip(susc_list, res_list):\n",
    "#    textfile.write(res + ',' '1' + \"\\n\")\n",
    "#    textfile.write(susc + ',' '0' + \"\\n\")\n",
    "#textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = open(\"/home/mashjunior/Desktop/FactorEmbeddings/embedding_eperiments/tt_embeddings/sentiment/TB_contig.tsv\", \"r\")\n",
    "#lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for line in lines:\n",
    " #   if line == \" \":\n",
    " #       print(\"Empty Line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "#print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--embedding', \n",
    "    default='tt',\n",
    "    choices=['tt', 'tr', 'full'],\n",
    "    type=str)\n",
    "parser.add_argument('--ranks', type=int, default=8)\n",
    "parser.add_argument('--d', type=int, default=3)\n",
    "parser.add_argument('--embed_dim', type=int, default=64)\n",
    "parser.add_argument('--voc_dim', default=250, type=int)\n",
    "parser.add_argument('--lr', default=5e-4)\n",
    "parser.add_argument('--gpu', default='', type=str)\n",
    "parser.add_argument('--hidden_dim', default=128, type=int)\n",
    "parser.add_argument('--n_epochs',  default=10, type=int)\n",
    "parser.add_argument('--fout',  default=\"logdir/\", type=str)\n",
    "parser.add_argument('--dropout', default=0.5, type=float)\n",
    "parser.add_argument(\n",
    "    '--dataset',\n",
    "    default='TB',\n",
    "    type=str)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embedding == 'tt':\n",
    "    tt = \"tt\"\n",
    "elif args.embedding == 'tr':\n",
    "    tt = 'tr'\n",
    "else:             \n",
    "    tt = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"{args.dataset}-dim_{args.embed_dim}-d_{args.d}-ranks_{args.ranks}-{tt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=args.gpu\n",
    "import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import t3nsor as t3\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.optim as optim\n",
    "from models import LSTM_Classifier\n",
    "#from utils import binary_accuracy, train, evaluate\n",
    "import pickle\n",
    "import random\n",
    "import spacy\n",
    "from spacy.cli.download import download\n",
    "random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the dataset\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"Building the dataset\")\n",
    "data_path = '/home/mashjunior/Desktop/FactorEmbeddings/embedding_eperiments/tt_embeddings/sentiment/TB_contig.tsv'\n",
    "TEXT = data.Field(tokenize='spacy', fix_length=100)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "fields = [('text', TEXT),('label', LABEL)]\n",
    "training_data = data.TabularDataset(\n",
    "    path=data_path,\n",
    "    format='csv',\n",
    "    fields=fields,\n",
    "    skip_header=True,\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = training_data.split(split_ratio=0.7, random_state=random.seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, test_data = val_data.split(split_ratio=0.2, random_state=random.seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 1\n",
    "#train_data, test_ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "#test_list = list(test_)\n",
    "#random.shuffle(test_list)\n",
    "#test_data_ = test_list[:12500]\n",
    "#val_data_ = test_list[12500:]\n",
    "#train_data = data.dataset.Dataset(train_data, fields=[('text', TEXT), ('label', LABEL)])\n",
    "valid_data = data.dataset.Dataset(val_data, fields=fields)\n",
    "test_data = data.dataset.Dataset(test_data, fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(ex):\n",
    "    return len(ex.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=args.voc_dim - 2)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)\n",
    "\n",
    "valid_iterator.sort_key = sort_key\n",
    "test_iterator.sort_key = sort_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = args.embed_dim\n",
    "HIDDEN_DIM = args.hidden_dim\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = args.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vocab_size = len(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_Classifier(embedding_dim=EMBEDDING_DIM,\n",
    "                             hidden_dim=HIDDEN_DIM,\n",
    "                             output_dim=OUTPUT_DIM,\n",
    "                             n_layers=N_LAYERS,\n",
    "                             bidirectional=BIDIRECTIONAL,\n",
    "                             dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embedding == 'tt':\n",
    "        embed_model = t3.TTEmbedding(\n",
    "            voc_size=INPUT_DIM,\n",
    "            emb_size=EMBEDDING_DIM,\n",
    "            auto_shapes=True,\n",
    "            auto_shape_mode='mixed',\n",
    "            d=args.d,\n",
    "            tt_rank=args.ranks,\n",
    "            padding_idx=1\n",
    "        )\n",
    "        compression_rate = INPUT_DIM * EMBEDDING_DIM / embed_model.tt_matrix.dof\n",
    "elif args.embedding == 'tr':\n",
    "        embed_model = t3.TREmbedding(\n",
    "            voc_size=INPUT_DIM,\n",
    "            emb_size=EMBEDDING_DIM,\n",
    "            auto_shapes=True,\n",
    "            auto_shape_mode='mixed',\n",
    "            d=args.d,\n",
    "            tr_rank=args.ranks,\n",
    "            padding_idx=1\n",
    "        )\n",
    "        compression_rate = INPUT_DIM * EMBEDDING_DIM / embed_model.tr_matrix.dof\n",
    "else:\n",
    "    embed_model = nn.Embedding(\n",
    "        num_embeddings=INPUT_DIM,\n",
    "        embedding_dim=EMBEDDING_DIM\n",
    "    )\n",
    "    compression_rate = 1.0\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, target):\n",
    "    labels = target.type(torch.LongTensor).to(logits.device)\n",
    "    return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(embed_model, lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    if len(preds.shape) == 1:\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    else:\n",
    "        rounded_preds = preds.argmax(1)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        dtype = torch.LongTensor\n",
    "    elif isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        device = batch.text.device\n",
    "        labels = batch.label.type(dtype).to(device)\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        B = batch.label.shape[0]\n",
    "\n",
    "        epoch_loss += B * loss.item()\n",
    "        epoch_acc += B * acc.item()\n",
    "\n",
    "        total_len += B\n",
    "\n",
    "\n",
    "        if i > len(iterator):\n",
    "            break\n",
    "\n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        dtype = torch.LongTensor\n",
    "    elif isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            device = batch.text.device\n",
    "            labels = batch.label.type(dtype).to(device)\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            B = batch.label.shape[0]\n",
    "\n",
    "            epoch_loss += B * loss.item()\n",
    "            epoch_acc += B * acc.item()\n",
    "            total_len += B\n",
    "\n",
    "            if i > len(iterator):\n",
    "                break\n",
    "\n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == 'TB':\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = criterion.to(device)\n",
    "#elif args.dataset[:3] == 'sst':\n",
    "#    criterion = nn.CrossEntropyLoss()\n",
    "    #criterion = criterion.to(device)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): TTEmbedding(\n",
      "    (parameters): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 1x5x4x8]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 8x6x4x8]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 8x10x4x1]\n",
      "    )\n",
      "  )\n",
      "  (1): LSTM_Classifier(\n",
      "    (rnn): LSTM(64, 128, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "print(model)\n",
    "N_EPOCHS = args.n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*5*4*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {\n",
    "    'compression_rate':compression_rate,\n",
    "    'train_loss':[], 'test_loss':[], 'valid_loss':[],\n",
    "    'train_acc':[], 'test_acc':[], 'valid_acc':[]}\n",
    "best_result = {\n",
    "    \"epoch\": 0, \"train_acc\": 0, \"valid_acc\": 0, \"train_acc\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 1.00 | Train Loss: 0.694 | Train Acc: 50.36% | Val. Loss: 0.694 | Val. Acc: 45.83% | Test Loss: 0.692 | Test Acc: 48.96% |\n",
      "TEST ACCURACY: 48.96\n",
      "| Epoch: 2.00 | Train Loss: 0.690 | Train Acc: 50.71% | Val. Loss: 0.694 | Val. Acc: 45.83% | Test Loss: 0.691 | Test Acc: 48.96% |\n",
      "TEST ACCURACY: 48.96\n",
      "| Epoch: 3.00 | Train Loss: 0.690 | Train Acc: 55.71% | Val. Loss: 0.692 | Val. Acc: 50.00% | Test Loss: 0.690 | Test Acc: 51.04% |\n",
      "TEST ACCURACY: 51.04\n",
      "| Epoch: 4.00 | Train Loss: 0.686 | Train Acc: 60.36% | Val. Loss: 0.687 | Val. Acc: 66.67% | Test Loss: 0.685 | Test Acc: 73.96% |\n",
      "TEST ACCURACY: 73.96\n",
      "| Epoch: 5.00 | Train Loss: 0.682 | Train Acc: 64.29% | Val. Loss: 0.681 | Val. Acc: 66.67% | Test Loss: 0.678 | Test Acc: 76.04% |\n",
      "TEST ACCURACY: 73.96\n",
      "| Epoch: 6.00 | Train Loss: 0.682 | Train Acc: 57.14% | Val. Loss: 0.674 | Val. Acc: 54.17% | Test Loss: 0.665 | Test Acc: 54.17% |\n",
      "TEST ACCURACY: 73.96\n",
      "| Epoch: 7.00 | Train Loss: 0.661 | Train Acc: 62.50% | Val. Loss: 0.650 | Val. Acc: 70.83% | Test Loss: 0.643 | Test Acc: 75.00% |\n",
      "TEST ACCURACY: 75.0\n",
      "| Epoch: 8.00 | Train Loss: 0.627 | Train Acc: 68.93% | Val. Loss: 0.623 | Val. Acc: 54.17% | Test Loss: 0.589 | Test Acc: 55.21% |\n",
      "TEST ACCURACY: 75.0\n",
      "| Epoch: 9.00 | Train Loss: 0.530 | Train Acc: 79.64% | Val. Loss: 0.594 | Val. Acc: 62.50% | Test Loss: 0.505 | Test Acc: 67.71% |\n",
      "TEST ACCURACY: 75.0\n",
      "| Epoch: 10.00 | Train Loss: 0.449 | Train Acc: 78.57% | Val. Loss: 0.390 | Val. Acc: 100.00% | Test Loss: 0.330 | Test Acc: 98.96% |\n",
      "TEST ACCURACY: 98.96\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    log['train_loss'].append(train_loss)\n",
    "    log['test_loss'].append(test_loss)\n",
    "    log['train_acc'].append(train_acc)\n",
    "    log['test_acc'].append(test_acc)\n",
    "    log['valid_acc'].append(valid_acc)\n",
    "    log['valid_loss'].append(valid_loss)\n",
    "\n",
    "    if best_result[\"valid_acc\"] < valid_acc:\n",
    "        best_result[\"epoch\"] = epoch\n",
    "        best_result[\"train_acc\"] = train_acc\n",
    "        best_result[\"valid_acc\"] = valid_acc\n",
    "        best_result[\"test_acc\"] = test_acc\n",
    "\n",
    "    #if args.fout is not None:\n",
    "    #    with open(args.fout+f\"{model_name}-best.pkl\", 'wb') as f:\n",
    "    #        pickle.dump(best_result, f)\n",
    "    print(f'| Epoch: {epoch+1:.2f} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% | Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')\n",
    "    print (\"TEST ACCURACY:\", np.round(best_result[\"test_acc\"] * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compression_rate': 7.936507936507937,\n",
       " 'train_loss': [0.6938424331801278,\n",
       "  0.6938001275062561,\n",
       "  0.6901356066976275,\n",
       "  0.6901510562215533,\n",
       "  0.6858684931482587,\n",
       "  0.681563379083361,\n",
       "  0.6820732917104448,\n",
       "  0.6612920267241341,\n",
       "  0.6273076891899109,\n",
       "  0.5302114622933524,\n",
       "  0.4490554256098611],\n",
       " 'test_loss': [0.695839007695516,\n",
       "  0.6924671729405721,\n",
       "  0.6914886832237244,\n",
       "  0.6897042393684387,\n",
       "  0.685492992401123,\n",
       "  0.6779112219810486,\n",
       "  0.6650245189666748,\n",
       "  0.6427273154258728,\n",
       "  0.5886634588241577,\n",
       "  0.5051289598147074,\n",
       "  0.3303088943163554],\n",
       " 'valid_loss': [0.6998911499977112,\n",
       "  0.6943073272705078,\n",
       "  0.6936776638031006,\n",
       "  0.6923295855522156,\n",
       "  0.6874141693115234,\n",
       "  0.6806931495666504,\n",
       "  0.6743363738059998,\n",
       "  0.6495236754417419,\n",
       "  0.6231448650360107,\n",
       "  0.5936792492866516,\n",
       "  0.3901023864746094],\n",
       " 'train_acc': [0.507142858845847,\n",
       "  0.5035714294229235,\n",
       "  0.5071428571428571,\n",
       "  0.5571428571428572,\n",
       "  0.6035714302744184,\n",
       "  0.6428571428571429,\n",
       "  0.5714285722800664,\n",
       "  0.6249999982970101,\n",
       "  0.6892857159887041,\n",
       "  0.7964285714285714,\n",
       "  0.7857142857142857],\n",
       " 'test_acc': [0.4895833333333333,\n",
       "  0.4895833333333333,\n",
       "  0.4895833333333333,\n",
       "  0.5104166666666666,\n",
       "  0.7395833333333334,\n",
       "  0.7604166666666666,\n",
       "  0.5416666666666666,\n",
       "  0.75,\n",
       "  0.5520833333333334,\n",
       "  0.6770833333333334,\n",
       "  0.9895833333333334],\n",
       " 'valid_acc': [0.4583333432674408,\n",
       "  0.4583333432674408,\n",
       "  0.4583333432674408,\n",
       "  0.5,\n",
       "  0.6666666865348816,\n",
       "  0.6666666865348816,\n",
       "  0.5416666865348816,\n",
       "  0.7083333134651184,\n",
       "  0.5416666865348816,\n",
       "  0.625,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 9,\n",
       " 'train_acc': 0.7857142857142857,\n",
       " 'valid_acc': 1.0,\n",
       " 'test_acc': 0.9895833333333334}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
