{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "#print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--embedding', \n",
    "    default='full',\n",
    "    choices=['tt', 'tr', 'full'],\n",
    "    type=str)\n",
    "parser.add_argument('--ranks', type=int, default=8)\n",
    "parser.add_argument('--d', type=int, default=3)\n",
    "parser.add_argument('--embed_dim', type=int, default=64)\n",
    "parser.add_argument('--voc_dim', default=250, type=int)\n",
    "parser.add_argument('--lr', default=5e-4)\n",
    "parser.add_argument('--gpu', default='', type=str)\n",
    "parser.add_argument('--hidden_dim', default=128, type=int)\n",
    "parser.add_argument('--n_epochs',  default=10, type=int)\n",
    "parser.add_argument('--fout',  default=\"logdir/\", type=str)\n",
    "parser.add_argument('--dropout', default=0.5, type=float)\n",
    "parser.add_argument(\n",
    "    '--dataset',\n",
    "    default='TB',\n",
    "    type=str)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embedding == 'tt':\n",
    "    tt = \"tt\"\n",
    "elif args.embedding == 'tr':\n",
    "    tt = 'tr'\n",
    "else:             \n",
    "    tt = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"{args.dataset}-dim_{args.embed_dim}-d_{args.d}-ranks_{args.ranks}-{tt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mashjunior/anaconda3/envs/james_tensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=args.gpu\n",
    "import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import t3nsor as t3\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.optim as optim\n",
    "from models import LSTM_Classifier\n",
    "#from utils import binary_accuracy, train, evaluate\n",
    "import pickle\n",
    "import random\n",
    "import spacy\n",
    "from spacy.cli.download import download\n",
    "random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the dataset\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"Building the dataset\")\n",
    "data_path = '/home/mashjunior/Desktop/FactorEmbeddings/embedding_eperiments/tt_embeddings/sentiment/contig_data.tsv'\n",
    "TEXT = data.Field(tokenize='spacy', fix_length=100)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "fields = [('text', TEXT),('label', LABEL)]\n",
    "training_data = data.TabularDataset(\n",
    "    path=data_path,\n",
    "    format='csv',\n",
    "    fields=fields,\n",
    "    skip_header=True,\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = training_data.split(split_ratio=0.7, random_state=random.seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, test_data = val_data.split(split_ratio=0.2, random_state=random.seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 1\n",
    "#train_data, test_ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "#test_list = list(test_)\n",
    "#random.shuffle(test_list)\n",
    "#test_data_ = test_list[:12500]\n",
    "#val_data_ = test_list[12500:]\n",
    "#train_data = data.dataset.Dataset(train_data, fields=[('text', TEXT), ('label', LABEL)])\n",
    "valid_data = data.dataset.Dataset(val_data, fields=fields)\n",
    "test_data = data.dataset.Dataset(test_data, fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(ex):\n",
    "    return len(ex.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=args.voc_dim - 2)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)\n",
    "\n",
    "valid_iterator.sort_key = sort_key\n",
    "test_iterator.sort_key = sort_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = args.embed_dim\n",
    "HIDDEN_DIM = args.hidden_dim\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = args.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vocab_size = len(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_Classifier(embedding_dim=EMBEDDING_DIM,\n",
    "                             hidden_dim=HIDDEN_DIM,\n",
    "                             output_dim=OUTPUT_DIM,\n",
    "                             n_layers=N_LAYERS,\n",
    "                             bidirectional=BIDIRECTIONAL,\n",
    "                             dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embedding == 'tt':\n",
    "        embed_model = t3.TTEmbedding(\n",
    "            voc_size=INPUT_DIM,\n",
    "            emb_size=EMBEDDING_DIM,\n",
    "            auto_shapes=True,\n",
    "            auto_shape_mode='mixed',\n",
    "            d=args.d,\n",
    "            tt_rank=args.ranks,\n",
    "            padding_idx=1\n",
    "        )\n",
    "        compression_rate = INPUT_DIM * EMBEDDING_DIM / embed_model.tt_matrix.dof\n",
    "elif args.embedding == 'tr':\n",
    "        embed_model = t3.TREmbedding(\n",
    "            voc_size=INPUT_DIM,\n",
    "            emb_size=EMBEDDING_DIM,\n",
    "            auto_shapes=True,\n",
    "            auto_shape_mode='mixed',\n",
    "            d=args.d,\n",
    "            tr_rank=args.ranks,\n",
    "            padding_idx=1\n",
    "        )\n",
    "        compression_rate = INPUT_DIM * EMBEDDING_DIM / embed_model.tr_matrix.dof\n",
    "else:\n",
    "    embed_model = nn.Embedding(\n",
    "        num_embeddings=INPUT_DIM,\n",
    "        embedding_dim=EMBEDDING_DIM\n",
    "    )\n",
    "    compression_rate = 1.0\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, target):\n",
    "    labels = target.type(torch.LongTensor).to(logits.device)\n",
    "    return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(embed_model, lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    if len(preds.shape) == 1:\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    else:\n",
    "        rounded_preds = preds.argmax(1)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        dtype = torch.LongTensor\n",
    "    elif isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        device = batch.text.device\n",
    "        labels = batch.label.type(dtype).to(device)\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        B = batch.label.shape[0]\n",
    "\n",
    "        epoch_loss += B * loss.item()\n",
    "        epoch_acc += B * acc.item()\n",
    "\n",
    "        total_len += B\n",
    "\n",
    "\n",
    "        if i > len(iterator):\n",
    "            break\n",
    "\n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        dtype = torch.LongTensor\n",
    "    elif isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            device = batch.text.device\n",
    "            labels = batch.label.type(dtype).to(device)\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            B = batch.label.shape[0]\n",
    "\n",
    "            epoch_loss += B * loss.item()\n",
    "            epoch_acc += B * acc.item()\n",
    "            total_len += B\n",
    "\n",
    "            if i > len(iterator):\n",
    "                break\n",
    "\n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == 'TB':\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = criterion.to(device)\n",
    "#elif args.dataset[:3] == 'sst':\n",
    "#    criterion = nn.CrossEntropyLoss()\n",
    "    #criterion = criterion.to(device)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Embedding(250, 64)\n",
      "  (1): LSTM_Classifier(\n",
      "    (rnn): LSTM(64, 128, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "print(model)\n",
    "N_EPOCHS = args.n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {\n",
    "    'compression_rate':compression_rate,\n",
    "    'train_loss':[], 'test_loss':[], 'valid_loss':[],\n",
    "    'train_acc':[], 'test_acc':[], 'valid_acc':[]}\n",
    "best_result = {\n",
    "    \"epoch\": 0, \"train_acc\": 0, \"valid_acc\": 0, \"train_acc\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 1.00 | Train Loss: 0.690 | Train Acc: 51.07% | Val. Loss: 0.690 | Val. Acc: 50.00% | Test Loss: 0.684 | Test Acc: 54.17% |\n",
      "TEST ACCURACY: 54.17\n",
      "| Epoch: 2.00 | Train Loss: 0.680 | Train Acc: 59.64% | Val. Loss: 0.667 | Val. Acc: 66.67% | Test Loss: 0.661 | Test Acc: 75.00% |\n",
      "TEST ACCURACY: 75.0\n",
      "| Epoch: 3.00 | Train Loss: 0.662 | Train Acc: 62.14% | Val. Loss: 0.657 | Val. Acc: 58.33% | Test Loss: 0.641 | Test Acc: 54.17% |\n",
      "TEST ACCURACY: 75.0\n",
      "| Epoch: 4.00 | Train Loss: 0.634 | Train Acc: 61.07% | Val. Loss: 0.623 | Val. Acc: 66.67% | Test Loss: 0.600 | Test Acc: 70.83% |\n",
      "TEST ACCURACY: 75.0\n",
      "| Epoch: 5.00 | Train Loss: 0.596 | Train Acc: 72.86% | Val. Loss: 0.616 | Val. Acc: 58.33% | Test Loss: 0.580 | Test Acc: 54.17% |\n",
      "TEST ACCURACY: 75.0\n",
      "| Epoch: 6.00 | Train Loss: 0.545 | Train Acc: 71.07% | Val. Loss: 0.475 | Val. Acc: 79.17% | Test Loss: 0.404 | Test Acc: 93.75% |\n",
      "TEST ACCURACY: 93.75\n",
      "| Epoch: 7.00 | Train Loss: 0.499 | Train Acc: 76.43% | Val. Loss: 0.420 | Val. Acc: 70.83% | Test Loss: 0.380 | Test Acc: 75.00% |\n",
      "TEST ACCURACY: 93.75\n",
      "| Epoch: 8.00 | Train Loss: 0.501 | Train Acc: 73.21% | Val. Loss: 0.460 | Val. Acc: 83.33% | Test Loss: 0.357 | Test Acc: 93.75% |\n",
      "TEST ACCURACY: 93.75\n",
      "| Epoch: 9.00 | Train Loss: 0.383 | Train Acc: 85.00% | Val. Loss: 0.524 | Val. Acc: 79.17% | Test Loss: 0.345 | Test Acc: 86.46% |\n",
      "TEST ACCURACY: 93.75\n",
      "| Epoch: 10.00 | Train Loss: 0.273 | Train Acc: 91.43% | Val. Loss: 0.197 | Val. Acc: 100.00% | Test Loss: 0.170 | Test Acc: 96.88% |\n",
      "TEST ACCURACY: 96.88\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    log['train_loss'].append(train_loss)\n",
    "    log['test_loss'].append(test_loss)\n",
    "    log['train_acc'].append(train_acc)\n",
    "    log['test_acc'].append(test_acc)\n",
    "    log['valid_acc'].append(valid_acc)\n",
    "    log['valid_loss'].append(valid_loss)\n",
    "\n",
    "    if best_result[\"valid_acc\"] < valid_acc:\n",
    "        best_result[\"epoch\"] = epoch\n",
    "        best_result[\"train_acc\"] = train_acc\n",
    "        best_result[\"valid_acc\"] = valid_acc\n",
    "        best_result[\"test_acc\"] = test_acc\n",
    "\n",
    "    #if args.fout is not None:\n",
    "    #    with open(args.fout+f\"{model_name}-best.pkl\", 'wb') as f:\n",
    "    #        pickle.dump(best_result, f)\n",
    "    print(f'| Epoch: {epoch+1:.2f} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% | Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')\n",
    "    print (\"TEST ACCURACY:\", np.round(best_result[\"test_acc\"] * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compression_rate': 1.0,\n",
       " 'train_loss': [0.6897736396108355,\n",
       "  0.6804030707904271,\n",
       "  0.6622858847890581,\n",
       "  0.6343153732163566,\n",
       "  0.5957407082830156,\n",
       "  0.544941406590598,\n",
       "  0.4992321653025491,\n",
       "  0.5006222171442849,\n",
       "  0.3826727219990322,\n",
       "  0.2734083767448153],\n",
       " 'test_loss': [0.6835191448529562,\n",
       "  0.6609715024630228,\n",
       "  0.6409523288408915,\n",
       "  0.599717398484548,\n",
       "  0.5798415144284567,\n",
       "  0.40420785546302795,\n",
       "  0.37961949904759723,\n",
       "  0.3573540349801381,\n",
       "  0.3450621763865153,\n",
       "  0.1702261765797933],\n",
       " 'valid_loss': [0.6895813941955566,\n",
       "  0.6674940586090088,\n",
       "  0.6571474671363831,\n",
       "  0.6232240796089172,\n",
       "  0.6163012385368347,\n",
       "  0.47503402829170227,\n",
       "  0.42000821232795715,\n",
       "  0.46019086241722107,\n",
       "  0.5238416790962219,\n",
       "  0.19741792976856232],\n",
       " 'train_acc': [0.5107142857142857,\n",
       "  0.5964285697255816,\n",
       "  0.6214285731315613,\n",
       "  0.6107142857142858,\n",
       "  0.7285714302744184,\n",
       "  0.7107142840112959,\n",
       "  0.7642857142857142,\n",
       "  0.7321428554398673,\n",
       "  0.8499999982970101,\n",
       "  0.9142857125827244],\n",
       " 'test_acc': [0.5416666666666666,\n",
       "  0.75,\n",
       "  0.5416666666666666,\n",
       "  0.7083333333333334,\n",
       "  0.5416666666666666,\n",
       "  0.9375,\n",
       "  0.75,\n",
       "  0.9375,\n",
       "  0.8645833333333334,\n",
       "  0.96875],\n",
       " 'valid_acc': [0.5,\n",
       "  0.6666666865348816,\n",
       "  0.5833333134651184,\n",
       "  0.6666666865348816,\n",
       "  0.5833333134651184,\n",
       "  0.7916666865348816,\n",
       "  0.7083333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.7916666865348816,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 9,\n",
       " 'train_acc': 0.9142857125827244,\n",
       " 'valid_acc': 1.0,\n",
       " 'test_acc': 0.96875}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
