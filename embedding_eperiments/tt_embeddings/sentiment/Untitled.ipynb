{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--embedding', \n",
    "    default='tt',\n",
    "    choices=['tt', 'tr', 'full'],\n",
    "    type=str)\n",
    "parser.add_argument('--ranks', type=int, default=8)\n",
    "parser.add_argument('--d', type=int, default=3)\n",
    "parser.add_argument('--embed_dim', type=int, default=64)\n",
    "parser.add_argument('--voc_dim', default=50, type=int)\n",
    "parser.add_argument('--lr', default=5e-4)\n",
    "parser.add_argument('--gpu', default='', type=str)\n",
    "parser.add_argument('--hidden_dim', default=128, type=int)\n",
    "parser.add_argument('--n_epochs',  default=10, type=int)\n",
    "parser.add_argument('--fout',  default=\"logdir/\", type=str)\n",
    "parser.add_argument('--dropout', default=0.5, type=float)\n",
    "parser.add_argument(\n",
    "    '--dataset',\n",
    "    default='imdb',\n",
    "    choices=['imdb', 'sst3', 'sst5'],\n",
    "    type=str)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embedding == 'tt':\n",
    "    tt = \"tt\"\n",
    "elif args.embedding == 'tr':\n",
    "    tt = 'tr'\n",
    "else:             \n",
    "    tt = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"{args.dataset}-dim_{args.embed_dim}-d_{args.d}-ranks_{args.ranks}-{tt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=args.gpu\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import t3nsor as t3\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.optim as optim\n",
    "from models import LSTM_Classifier\n",
    "#from utils import binary_accuracy, train, evaluate\n",
    "import pickle\n",
    "import random\n",
    "import en_core_web_sm\n",
    "\n",
    "random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', fix_length=100)\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building dataset...')\n",
    "if args.dataset == 'imdb':\n",
    "    OUTPUT_DIM = 1\n",
    "    train_data, test_ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    test_list = list(test_)\n",
    "    random.shuffle(test_list)\n",
    "    test_data_ = test_list[:12500]\n",
    "    val_data_ = test_list[12500:]\n",
    "    valid_data = data.dataset.Dataset(\n",
    "        val_data_, fields=[('text', TEXT), ('label', LABEL)])\n",
    "    test_data = data.dataset.Dataset(\n",
    "        test_data_, fields=[('text', TEXT), ('label', LABEL)])\n",
    "elif args.dataset[:3] == 'sst':\n",
    "    OUTPUT_DIM = int(args.dataset[3])\n",
    "    fine_grained = (OUTPUT_DIM == 5)\n",
    "    train_data, valid_data, test_data = datasets.SST.splits(\n",
    "        TEXT, LABEL, fine_grained=fine_grained)\n",
    "else:\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(ex):\n",
    "    return len(ex.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=args.voc_dim - 2)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)\n",
    "\n",
    "valid_iterator.sort_key = sort_key\n",
    "test_iterator.sort_key = sort_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = args.embed_dim\n",
    "HIDDEN_DIM = args.hidden_dim\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = args.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vocab_size = len(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_Classifier(embedding_dim=EMBEDDING_DIM,\n",
    "                             hidden_dim=HIDDEN_DIM,\n",
    "                             output_dim=OUTPUT_DIM,\n",
    "                             n_layers=N_LAYERS,\n",
    "                             bidirectional=BIDIRECTIONAL,\n",
    "                             dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embedding == 'tt':\n",
    "        embed_model = t3.TTEmbedding(\n",
    "            voc_size=INPUT_DIM,\n",
    "            emb_size=EMBEDDING_DIM,\n",
    "            auto_shapes=True,\n",
    "            auto_shape_mode='mixed',\n",
    "            d=args.d,\n",
    "            tt_rank=args.ranks,\n",
    "            padding_idx=1\n",
    "        )\n",
    "        compression_rate = INPUT_DIM * EMBEDDING_DIM / embed_model.tt_matrix.dof\n",
    "elif args.embedding == 'tr':\n",
    "        embed_model = t3.TREmbedding(\n",
    "            voc_size=INPUT_DIM,\n",
    "            emb_size=EMBEDDING_DIM,\n",
    "            auto_shapes=True,\n",
    "            auto_shape_mode='mixed',\n",
    "            d=args.d,\n",
    "            tr_rank=args.ranks,\n",
    "            padding_idx=1\n",
    "        )\n",
    "        compression_rate = INPUT_DIM * EMBEDDING_DIM / embed_model.tr_matrix.dof\n",
    "else:\n",
    "    embed_model = nn.Embedding(\n",
    "        num_embeddings=INPUT_DIM,\n",
    "        embedding_dim=EMBEDDING_DIM\n",
    "    )\n",
    "    compression_rate = 1.0\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, target):\n",
    "    labels = target.type(torch.LongTensor).to(logits.device)\n",
    "    return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(embed_model, lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    if len(preds.shape) == 1:\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    else:\n",
    "        rounded_preds = preds.argmax(1)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        dtype = torch.LongTensor\n",
    "    elif isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        device = batch.text.device\n",
    "        labels = batch.label.type(dtype).to(device)\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        B = batch.label.shape[0]\n",
    "\n",
    "        epoch_loss += B * loss.item()\n",
    "        epoch_acc += B * acc.item()\n",
    "\n",
    "        total_len += B\n",
    "\n",
    "\n",
    "        if i > len(iterator):\n",
    "            break\n",
    "\n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        dtype = torch.LongTensor\n",
    "    elif isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            device = batch.text.device\n",
    "            labels = batch.label.type(dtype).to(device)\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            B = batch.label.shape[0]\n",
    "\n",
    "            epoch_loss += B * loss.item()\n",
    "            epoch_acc += B * acc.item()\n",
    "            total_len += B\n",
    "\n",
    "            if i > len(iterator):\n",
    "                break\n",
    "\n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == 'imdb':\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    #criterion = criterion.to(device)\n",
    "elif args.dataset[:3] == 'sst':\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #criterion = criterion.to(device)\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "print(model)\n",
    "N_EPOCHS = args.n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {\n",
    "    'compression_rate':compression_rate,\n",
    "    'train_loss':[], 'test_loss':[], 'valid_loss':[],\n",
    "    'train_acc':[], 'test_acc':[], 'valid_acc':[]}\n",
    "best_result = {\n",
    "    \"epoch\": 0, \"train_acc\": 0, \"valid_acc\": 0, \"train_acc\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss, train_acc = train(model, train_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    log['train_loss'].append(train_loss)\n",
    "    log['test_loss'].append(test_loss)\n",
    "    log['train_acc'].append(train_acc)\n",
    "    log['test_acc'].append(test_acc)\n",
    "    log['valid_acc'].append(valid_acc)\n",
    "    log['valid_loss'].append(valid_loss)\n",
    "\n",
    "    if best_result[\"valid_acc\"] < valid_acc:\n",
    "        best_result[\"epoch\"] = epoch\n",
    "        best_result[\"train_acc\"] = train_acc\n",
    "        best_result[\"valid_acc\"] = valid_acc\n",
    "        best_result[\"test_acc\"] = test_acc\n",
    "\n",
    "    #if args.fout is not None:\n",
    "    #    with open(args.fout+f\"{model_name}-best.pkl\", 'wb') as f:\n",
    "    #        pickle.dump(best_result, f)\n",
    "    print(f'| Epoch: {epoch+1:0.2f} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% | Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')\n",
    "    print (\"TEST ACCURACY:\", np.round(best_result[\"test_acc\"] * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
